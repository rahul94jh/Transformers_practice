{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transformer_skelton_pytorch",
      "provenance": [],
      "authorship_tag": "ABX9TyOwiSfSo8t86j7IsedrohrT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul94jh/Transformers_practice/blob/main/Transformer_skelton_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXmlcCoLPcz0"
      },
      "source": [
        "based on article at https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y67-7NPGPGbL"
      },
      "source": [
        "from torch import Tensor\r\n",
        "import torch.nn.functional as f\r\n",
        "import torch\r\n",
        "from torch import nn\r\n"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK_lQwVxQK-L"
      },
      "source": [
        "Q, K, and V are batches of matrices, each with shape (batch_size, seq_length, num_features). Multiplying the query (Q) and key (K) arrays results in a (batch_size, seq_length, seq_length)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FgIgmWHPqc6"
      },
      "source": [
        "def scaled_dot_product_attention(query: Tensor, key: Tensor, value: Tensor) -> Tensor:\r\n",
        "    temp = query.bmm(key.transpose(1, 2))\r\n",
        "    scale = query.size(-1) ** 0.5\r\n",
        "    softmax = f.softmax(temp / scale, dim=-1)\r\n",
        "    return softmax.bmm(value)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-3CgQC2QLt9"
      },
      "source": [
        "MatMul operations are translated to torch.bmm in PyTorch. That’s because Q, K, and V (query, key, and value arrays) are batches of matrices, each with shape (batch_size, sequence_length, num_features). Batch matrix multiplication is only performed over the last two dimensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xbq1OWZRXJH"
      },
      "source": [
        "class AttentionHead(nn.Module):\r\n",
        "    def __init__(self, dim_in: int, dim_k: int, dim_v: int):\r\n",
        "        super().__init__()\r\n",
        "        self.q = nn.Linear(dim_in, dim_k)\r\n",
        "        self.k = nn.Linear(dim_in, dim_k)\r\n",
        "        self.v = nn.Linear(dim_in, dim_v)\r\n",
        "\r\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\r\n",
        "        return scaled_dot_product_attention(self.q(query), self.k(key), self.v(value))\r\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svK-PFCXS1pC"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\r\n",
        "    def __init__(self, num_heads: int, dim_in: int, dim_k: int, dim_v: int):\r\n",
        "        super().__init__()\r\n",
        "        self.heads = nn.ModuleList(\r\n",
        "            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\r\n",
        "        )\r\n",
        "        self.linear = nn.Linear(num_heads * dim_v, dim_in)\r\n",
        "\r\n",
        "    def forward(self, query: Tensor, key: Tensor, value: Tensor) -> Tensor:\r\n",
        "        return self.linear(\r\n",
        "            torch.cat([h(query, key, value) for h in self.heads], dim=-1)\r\n",
        "        )"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5nAGqZfUt8N"
      },
      "source": [
        "Each attention head computes its own query, key, and value arrays, and then applies scaled dot-product attention. Conceptually, this means each head can attend to a different part of the input sequence, independent of the others. Increasing the number of attention heads allows us to “pay attention” to more parts of the sequence at once, which makes the model more powerful"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GD__wXgtVEEl"
      },
      "source": [
        "def position_encoding(\r\n",
        "    seq_len: int, dim_model: int, device: torch.device = torch.device(\"cpu\"),\r\n",
        ") -> Tensor:\r\n",
        "    pos = torch.arange(seq_len, dtype=torch.float, device=device).reshape(1, -1, 1)\r\n",
        "    dim = torch.arange(dim_model, dtype=torch.float, device=device).reshape(1, 1, -1)\r\n",
        "    phase = pos / 1e4 ** (dim // dim_model)\r\n",
        "\r\n",
        "    return torch.where(dim.long() % 2 == 0, torch.sin(phase), torch.cos(phase))\r\n"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYHedDD1V26f"
      },
      "source": [
        "def feed_forward(dim_input: int = 512, dim_feedforward: int = 2048) -> nn.Module:\r\n",
        "    return nn.Sequential(\r\n",
        "        nn.Linear(dim_input, dim_feedforward),\r\n",
        "        nn.ReLU(),\r\n",
        "        nn.Linear(dim_feedforward, dim_input),\r\n",
        "    )"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1LBpUsaVWYSR"
      },
      "source": [
        "class Residual(nn.Module):\r\n",
        "    def __init__(self, sublayer: nn.Module, dimension: int, dropout: float = 0.1):\r\n",
        "        super().__init__()\r\n",
        "        self.sublayer = sublayer\r\n",
        "        self.norm = nn.LayerNorm(dimension)\r\n",
        "        self.dropout = nn.Dropout(dropout)\r\n",
        "\r\n",
        "    def forward(self, *tensors: Tensor) -> Tensor:\r\n",
        "        # Assume that the \"value\" tensor is given last, so we can compute the\r\n",
        "        # residual.  This matches the signature of 'MultiHeadAttention'.\r\n",
        "        return self.norm(tensors[-1] + self.dropout(self.sublayer(*tensors)))"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wM0Li2IWcnC"
      },
      "source": [
        "class TransformerEncoderLayer(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self, \r\n",
        "        dim_model: int = 512, \r\n",
        "        num_heads: int = 6, \r\n",
        "        dim_feedforward: int = 2048, \r\n",
        "        dropout: float = 0.1, \r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        dim_k = dim_v = dim_model // num_heads\r\n",
        "        self.attention = Residual(\r\n",
        "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\r\n",
        "            dimension=dim_model,\r\n",
        "            dropout=dropout,\r\n",
        "        )\r\n",
        "        self.feed_forward = Residual(\r\n",
        "            feed_forward(dim_model, dim_feedforward),\r\n",
        "            dimension=dim_model,\r\n",
        "            dropout=dropout,\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, src: Tensor) -> Tensor:\r\n",
        "        src = self.attention(src, src, src)\r\n",
        "        return self.feed_forward(src)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x37yFzi-dJmm"
      },
      "source": [
        "class TransformerEncoder(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self, \r\n",
        "        num_layers: int = 6,\r\n",
        "        dim_model: int = 512, \r\n",
        "        num_heads: int = 8, \r\n",
        "        dim_feedforward: int = 2048, \r\n",
        "        dropout: float = 0.1, \r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        self.layers = nn.ModuleList([\r\n",
        "            TransformerEncoderLayer(dim_model, num_heads, dim_feedforward, dropout)\r\n",
        "            for _ in range(num_layers)\r\n",
        "        ])\r\n",
        "\r\n",
        "    def forward(self, src: Tensor) -> Tensor:\r\n",
        "        seq_len, dimension = src.size(1), src.size(2)\r\n",
        "        src += position_encoding(seq_len, dimension)\r\n",
        "        for layer in self.layers:\r\n",
        "            src = layer(src)\r\n",
        "\r\n",
        "        return src"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ww6HhOuOkMvN"
      },
      "source": [
        "class TransformerDecoderLayer(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self, \r\n",
        "        dim_model: int = 512, \r\n",
        "        num_heads: int = 6, \r\n",
        "        dim_feedforward: int = 2048, \r\n",
        "        dropout: float = 0.1, \r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        dim_k = dim_v = dim_model // num_heads\r\n",
        "        self.attention_1 = Residual(\r\n",
        "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\r\n",
        "            dimension=dim_model,\r\n",
        "            dropout=dropout,\r\n",
        "        )\r\n",
        "        self.attention_2 = Residual(\r\n",
        "            MultiHeadAttention(num_heads, dim_model, dim_k, dim_v),\r\n",
        "            dimension=dim_model,\r\n",
        "            dropout=dropout,\r\n",
        "        )\r\n",
        "        self.feed_forward = Residual(\r\n",
        "            feed_forward(dim_model, dim_feedforward),\r\n",
        "            dimension=dim_model,\r\n",
        "            dropout=dropout,\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\r\n",
        "        tgt = self.attention_1(tgt, tgt, tgt)\r\n",
        "        tgt = self.attention_2(memory, memory, tgt)\r\n",
        "        return self.feed_forward(tgt)"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBBBT1YhmNo5"
      },
      "source": [
        "class TransformerDecoder(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self, \r\n",
        "        num_layers: int = 6,\r\n",
        "        dim_model: int = 512, \r\n",
        "        num_heads: int = 8, \r\n",
        "        dim_feedforward: int = 2048, \r\n",
        "        dropout: float = 0.1, \r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        self.layers = nn.ModuleList([\r\n",
        "            TransformerDecoderLayer(dim_model, num_heads, dim_feedforward, dropout)\r\n",
        "            for _ in range(num_layers)\r\n",
        "        ])\r\n",
        "        self.linear = nn.Linear(dim_model, dim_model)\r\n",
        "\r\n",
        "    def forward(self, tgt: Tensor, memory: Tensor) -> Tensor:\r\n",
        "        seq_len, dimension = tgt.size(1), tgt.size(2)\r\n",
        "        tgt += position_encoding(seq_len, dimension)\r\n",
        "        for layer in self.layers:\r\n",
        "            tgt = layer(tgt, memory)\r\n",
        "\r\n",
        "        return torch.softmax(self.linear(tgt), dim=-1)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4KLVu92nP2A"
      },
      "source": [
        "class Transformer(nn.Module):\r\n",
        "    def __init__(\r\n",
        "        self, \r\n",
        "        num_encoder_layers: int = 6,\r\n",
        "        num_decoder_layers: int = 6,\r\n",
        "        dim_model: int = 512, \r\n",
        "        num_heads: int = 6, \r\n",
        "        dim_feedforward: int = 2048, \r\n",
        "        dropout: float = 0.1, \r\n",
        "        activation: nn.Module = nn.ReLU(),\r\n",
        "    ):\r\n",
        "        super().__init__()\r\n",
        "        self.encoder = TransformerEncoder(\r\n",
        "            num_layers=num_encoder_layers,\r\n",
        "            dim_model=dim_model,\r\n",
        "            num_heads=num_heads,\r\n",
        "            dim_feedforward=dim_feedforward,\r\n",
        "            dropout=dropout,\r\n",
        "        )\r\n",
        "        self.decoder = TransformerDecoder(\r\n",
        "            num_layers=num_decoder_layers,\r\n",
        "            dim_model=dim_model,\r\n",
        "            num_heads=num_heads,\r\n",
        "            dim_feedforward=dim_feedforward,\r\n",
        "            dropout=dropout,\r\n",
        "        )\r\n",
        "\r\n",
        "    def forward(self, src: Tensor, tgt: Tensor) -> Tensor:\r\n",
        "        return self.decoder(tgt, self.encoder(src))"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "akXd-l5DntOH"
      },
      "source": [
        "src = torch.rand(64, 16, 512)\r\n",
        "tgt = torch.rand(64, 16, 512)\r\n",
        "out = Transformer()(src, tgt)\r\n",
        "print(out.shape)\r\n",
        "# torch.Size([64, 16, 512])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUeQncXpn8qW"
      },
      "source": [
        "out = Transformer()\r\n",
        "out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns6jrYf82kix"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}